## Unit 1 Reflection – What? So What? Now What?

### **What?**

In Unit 1, I conducted an individual, practice-based project for a real client within a **System Engineering Project Management (SEPM)** context. The task focused on identifying why engineering projects fail and how such risks can be mitigated through structured design validation.
To ground my analysis in academic research, I applied **Agrawal et al. (2024)**’s software design error taxonomy, which classifies design-phase failures into four categories: cognitive-load, knowledge-based, organisational-influence, and process-related errors. I examined my client’s infrastructure transition through this framework, identifying how knowledge gaps, unrealistic deadlines, and missing design-review mechanisms could result in systemic project failure. The project combined real-world observation with academic evidence to produce actionable recommendations.

### **So What?**

This project demonstrated that **project failure is rarely caused by technology alone**; it arises primarily from human and organisational decisions made under constraint. Applying Agrawal’s taxonomy to a real client environment showed how even well-resourced transitions can fail when governance and validation processes are weak.
This insight has reshaped my understanding of SEPM: success depends equally on **process integrity, communication transparency, and governance accountability** as it does on technical competence. Conducting this project independently strengthened my analytical discipline, as I had to verify each assumption and ensure that every conclusion was evidence-based. It reinforced the value of connecting theory with professional practice — turning abstract models into operational frameworks that directly inform decision-making.

### **Now What?**

Looking ahead, I intend to formalise the **Design-Review-Gate** framework developed in this project into a repeatable governance model for future client engagements. This framework will embed validation criteria for encryption, data-handling, and non-duplication requirements before system deployment.
I will continue refining my **evidence-based project management approach**, integrating quantitative metrics (e.g., error frequency and mitigation cost) with qualitative insights from stakeholder reviews. Furthermore, I will focus on improving how I communicate complex risk structures to non-technical executives, ensuring that governance outcomes are both technically sound and managerially comprehensible.
This experience confirmed that excellence in SEPM relies on the synergy of **technical depth**, **analytical rigour**, and **strategic communication** — each fundamental to achieving sustainable and accountable engineering outcomes.

---

### **References**

Agrawal, T., Walia, G.S. and Anu, V.K. (2024) ‘Development of a software design error taxonomy: a systematic literature review’, *SN Computer Science*, 5(467). Available at: [https://doi.org/10.1007/s42979-024-02797-2](https://doi.org/10.1007/s42979-024-02797-2) (Accessed: 26 October 2025).

ENISA (2024) *Good practice guide for securely deploying governmental clouds*. European Union Agency for Cybersecurity. Available at: [https://www.enisa.europa.eu/publications](https://www.enisa.europa.eu/publications) (Accessed: 26 October 2025).

ISO/IEC (2022) *ISO/IEC 27017: Code of practice for information security controls based on ISO/IEC 27002 for cloud services*. Geneva: International Organization for Standardization.